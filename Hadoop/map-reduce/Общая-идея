########################### Общая схема ########################### 

Надо выполнить большие вычисления для которых необходимо использовать мощьности кластера машин.

Для этого существует парадигма Map-Reduce
Вычисления делятся на 2 фазы за которые отвечают 2 типа обработчиков:
1. Map (маперы)
2. Reduce (редъюсеры)

			==== Общая схема Map-Reduce ==== 

 Все данные при этом делятся на НЕЗАВИСИМЫЕ части - split (сплиты).
 Сплиты представляют собой либо целые HDFS блоки либо специальные splitable архивы.

 Каждая часть обрабатывается своим независисым обработчиком.
 Стараются делать так что-бы worker который будет работать со сплитом был на той-же машине что и данные это называется Data Locality,
 тогда передача данных воркеру будет быстрая так как не придется его никуда передавать по сети.

Каждый мапер записывает результат себе на диск в файлы, колличество файлов равно колличеству редъюсеров. Тоесть у каждого маппера 
получается по 1му файлу для каждого редъюсера. 
Результат вычисления маперов представляет из себя <Key, Value>.


Затем данные передаются редъюсерам. Данные с одинаковыми ключами из разных маперов должны придти в один редъюсер.

Процесс копирования из всех мапперов во все редъюсеры и происходит только когда все мапперы отработали и по сети и называется shuffle.

Маперы и редъюсеры работают независимо.
Когда маппер проделал свою работу он складывает результаты маппинга не в HDFS а на локальный диск(если диск сломался маппер все делает с начала).

В итоге в редъюсере должна получиться мапа с уникальным ключем и массивом значений. Т.е. агрегация значений для каждого ключа.

В каждом редъюсере получился массив значений Val на пример Key='c', Val={1,4,3} 
Этот массив мержется из Key='c', Val={1,4,3} к Key='c', Val=8 этот этап называется sort

Затем результат пишется в output файлы которые хранятся в HDFS. Колличество вайлов == колличеству редъюсеров
Количество редъюсеров как и формат выходных данных задается пользователем.

Чем больше редъюсеров тем мельче выходные файлы оптимально 500Мб до 1,5Гб для каждого редъюсера.

			==== Map-Reduce без Reduce ==== 

Может быть что фаза Reduce просто не требуется.

Или

Может быть Map-Reduce без Reduce если каждый маппер работает очень долго на пример больше суток тогда чтобы возспользоваться HDFS
можно записать результаты маппинга в output file на случай если диск выйдет из строя. И посылаем результат маппинга в новую Map-Reduce задачу
где фаза Map просто зачитывает готовые данные.

########################### Как именно ########################### 

Нужно:
1)Функцию map() + где лежать входные данные
2)Функцию reduce() + куда складывать результат
3)Все это передается в master который определяет как разбить данные на split-ты. Так же master следит за тем чтобы перезапускать 
мапперы если они упадут.
4)Маперы копируют промежуточные данные в файлы и когда все маперы отработали все данные начинают копироваться на редъюсеры.
5)Редъюсеры в процессе работы пишут выходные файлы в HDFS.
6)Master перезапускает редъюсеры если они падают.
7)Когда все выходные файлы готовы мастер сообщает пользователю.

########################### Реализация Map-Reduce в Hadoop ########################### 

На каждом сервере есть datanode daemon поток опбеспечивающий взаимодействие с HDFS.

Отдельный сервер с Namenode - хранит meta информацию о файлах. (Аттрибут HDFS)

Отдельный сервер на котором работет jobtracker - который отвечает за :
1)Где входные данные
2)Как их сплитить
3)На каких серверах нужно запускать воркеры о чем сообщает tasktracker на конкретном сервере
4)Следит за процессом выполнения задачи и сообщает клиенту
5)Следит за ошибками перезапускает задачи и восстанавливает данные

На каждой машине кластера есть tasktracker отвечает за:
1)Запуск конкретных воркеров на конкретном сервере.
2)Следит за процессом выполнения очереди воркеров и шлет ее jobtracker

###########################  Дополнительно ########################### 

Conbine - это локальный редюсер для определенного маппера (см. Расширенная-схема-img)
Удобно если в маперах много данных.

Partition - можно определить в какой редъюсер отправлять ключи. В том числе несколько разных ключей можно
отправитьв один редъюсер. (см. Расширенная-схема-img)

###########################  Функции ########################### 

Основные:

1. map
List<K2, V2> map(Map<K1, V1> src)

2.reduce
List<K3, V3> reduce(Map<K2, List<V2>> src)

Вспомогательные

1.
partition - распределяет ключи по редъюсерам
void partition(K2, V2, int reducerAmount)

K2, V2, - это ключь и значение с которыми должен будет работать редъюсер
reducerAmount общее количество редъюсеров

По умолчанию берет hash(K2) и модуль от колличества редъюсеров.
Дает возможно определить как для каждого ключа определять номер редъюсера.

2.
локальный редюсер не гарантируется что будет запущен НЕ ЗАКЛАДЫВАТЬСЯ НА ЛОГИКУ ЧТО ОН ТОЧНО СРАБОТАЕТ!
List<K2, V2> combine(<K2, V2> )
Т.к. данные все равно попадут на основной редъюсер, типы должны совпадать

 
